# =============================================================================
# backend/news_scraper.py
# =============================================================================

import re
from datetime import date, timedelta
from typing import List, Optional

import requests
from bs4 import BeautifulSoup

from backend.database import save_news
from backend.utils.logger import logger


# =============================================================================
# CATEGORY LOGIC
# =============================================================================

def map_tomsarkgh_category(title: str, description: str | None = None) -> str:
    """
    Keyword-based guess for Tomsarkgh events:
    - holiday_events: Õ†Õ¸Ö€ Õ¿Õ¡Ö€Õ« / Christmas / Õ¿Õ¸Õ¶Õ¡Õ®Õ¡Õ¼ Ö‡ Õ¡ÕµÕ¬Õ¶
    - culture: theatre, cinema, opera, ballet, dance, etc.
    - city: some explicitly cityâ€‘related things
    - events: everything else
    """
    t = (title or "").lower()
    d = (description or "").lower()
    text = f"{t} {d}"

    # --- Holiday / seasonal ---
    holiday_keywords = [
        "new year", "new-year", "new yearâ€™s", "new years",
        "christmas", "xmas",
        "Õ¶Õ¸Ö€ Õ¿Õ¡Ö€Õ«", "Õ¡Õ´Õ¡Õ¶Õ¸Ö€", "Õ¿Õ¸Õ¶Õ¡Õ®Õ¡Õ¼", "Õ½Õ¸Ö‚Ö€Õ¢ Õ®Õ¶Õ¸Ö‚Õ¶Õ¤",
        "christmas market", "christmas fair",
    ]
    if any(k in text for k in holiday_keywords):
        return "holiday_events"

    # --- Culture (art) ---
    culture_keywords = [
        # theatre / performance
        "theatre", "theater", "Õ©Õ¡Õ¿Ö€Õ¸Õ¶", "performance",
        "play", "dramatic", "drama",
        # opera / ballet / dance
        "opera", "Ö…ÕºÕ¥Ö€Õ¡", "ballet", "Õ¢Õ¡Õ¬Õ¥Õ¿",
        "dance show", "dance performance", "ÕºÕ¡Ö€Õ¡ÕµÕ«Õ¶",
        # cinema / films
        "cinema", "film", "movie", "screening", "premiere", "Õ¯Õ«Õ¶Õ¸",
        # music / concerts of artistic type
        "symphony", "orchestra", "ensemble", "quartet", "choir",
        "classical music", "chamber music",
    ]
    if any(k in text for k in culture_keywords):
        return "culture"

    # --- Explicitly city-related ---
    city_keywords = [
        "city day", "Ö„Õ¡Õ²Õ¡Ö„Õ« Õ¿Õ¸Õ¶",
        "city tour", "Ö„Õ¡Õ²Õ¡Ö„Õ¡ÕµÕ«Õ¶ Õ¦Õ¢Õ¸Õ½Õ¡Õ¶Ö„",
        "Ö„Õ¡Õ²Õ¡Ö„Õ¡ÕºÕ¥Õ¿Õ¡Ö€Õ¡Õ¶", "city hall",
    ]
    if any(k in text for k in city_keywords):
        return "city"

    # Default guess
    return "events"


def final_category_from_source(
    base_category: str, title: str, description: str | None
) -> str:
    """
    Combine EventTypeâ€‘Õ«Ö Õ¥Õ¯Õ¡Õ® base_category + keywordâ€‘based guessed category.

    Õ‘Õ¡Õ¶Õ¯Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¤ Õ§Ö€Õ
    - 41 / seasonal â†’ holiday_events
    - 1,6,12,21 â†’ culture Õ´Õ«Õ¡ÕµÕ¶ Õ¡Ö€Õ¾Õ¥Õ½Õ¿Õ« Õ°Õ¡Õ´Õ¡Ö€
    - 7 â†’ city
    - 16,31,54,10,2 Ö‡ Õ´Õ¶Õ¡ÖÕ¡Õ®Õ¨ â†’ events (generic Õ´Õ«Õ»Õ¸ÖÕ¡Õ¼Õ¸Ö‚Õ´Õ¶Õ¥Ö€)
    """
    guessed = map_tomsarkgh_category(title, description)

    # 1) Seasonal Õ´Õ«Õ·Õ¿ holiday_events
    if base_category == "holiday_events" or guessed == "holiday_events":
        return "holiday_events"

    # 2) Õ„Õ¡Ö„Õ¸Ö‚Ö€ Õ¡Ö€Õ¾Õ¥Õ½Õ¿Õ« EventType-Õ¥Ö€Õ Õ´Õ«Õ·Õ¿ culture
    if base_category == "culture":
        return "culture"

    # 3) Õ”Õ¡Õ²Õ¡Ö„Õ¡ÕµÕ«Õ¶
    if base_category == "city":
        return "city"

    # 4) Õ„Õ¶Õ¡ÖÕ¡Õ®Õ¨Õ events (Õ¯Ö€Õ¯Õ¥Õ½, stand-up, club, pop, concerts, uncategorized)
    return "events"


# =============================================================================
# CONSTANTS
# =============================================================================

BASE_TOMSARKGH_URL = "https://www.tomsarkgh.am"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

# Tomsarkgh EventType IDs â†’ base AskYerevan categories
# Õ€Õ½Õ¿Õ¡Õ¯ Õ¤Õ¡Õ½Õ¡Õ¾Õ¸Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¤.
TOMSARKGH_CATEGORIES = {
    16: "events",         # Ô¿Ö€Õ¯Õ¥Õ½
    54: "events",         # Standâ€‘up
    31: "events",         # Ô±Õ¯Õ¸Ö‚Õ´Õ¢/ÖƒÕ¡Õ¢
    10: "events",         # ÕŠÕ¸Õº
    2:  "events",         # Ô¿Õ¸Õ¶ÖÕ¥Ö€Õ¿
    41: "holiday_events", # ÕÕ¡Ö€Õ¾Õ¡ Õ¿Õ¸Õ¶Õ¥Ö€ (seasonal only)
    1:  "culture",        # Ô¹Õ¡Õ¿Ö€Õ¸Õ¶
    6:  "culture",        # Ô¿Õ«Õ¶Õ¸
    12: "culture",        # Õ•ÕºÕ¥Ö€Õ¡â€‘Õ¢Õ¡Õ¬Õ¥Õ¿
    21: "culture",        # ÕŠÕ¡Ö€
    7:  "city",           # Õ”Õ¡Õ²Õ¡Ö„Õ¡ÕµÕ«Õ¶
}


# =============================================================================
# HELPERS
# =============================================================================

def _safe_text(el) -> str:
    """Extract clean text from BeautifulSoup element."""
    return el.get_text(strip=True) if el else ""


def _full_text(soup: BeautifulSoup) -> str:
    """Full visible text of the page (for regex fallbacks)."""
    return soup.get_text(separator="\n", strip=True)


# =============================================================================
# LIST PAGE â†’ EVENT URLS
# =============================================================================

def fetch_tomsarkgh_events(event_type: int, days_ahead: int = 7) -> List[str]:
    """
    Fetch event URLs from Tomsarkgh list endpoint by EventType and date range.
    Uses /list?EventType[]=...&startFrom=..&startTo=...
    """
    today = date.today()
    start = today.strftime("%m/%d/%Y")
    end = (today + timedelta(days=days_ahead)).strftime("%m/%d/%Y")

    params = {
        "EventType[]": str(event_type),
        "startFrom": start,
        "startTo": end,
    }

    try:
        logger.info(f"ğŸ“‹ Tomsarkgh list: type={event_type}, {start}â†’{end}")
        resp = requests.get(
            f"{BASE_TOMSARKGH_URL}/list",
            params=params,
            timeout=15,
            headers=HEADERS,
        )
        resp.raise_for_status()
    except Exception as e:
        logger.error(f"âŒ Tomsarkgh list error (type={event_type}): {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    links: List[str] = []

    for a in soup.select("a[href*='/hy/event/']"):
        href = a.get("href", "").strip()
        if not href or "/hy/event/" not in href:
            continue
        full_url = href if href.startswith("http") else BASE_TOMSARKGH_URL + href
        if full_url not in links:
            links.append(full_url)

    logger.info(f"âœ… Found {len(links)} events for type={event_type}")
    return links[:20]


# =============================================================================
# EVENT PAGE PARSER
# =============================================================================

def _parse_event_datetime(soup: BeautifulSoup) -> (str, str):
    """
    Return (eventdate, eventtime) strings.
    """
    meta = soup.select_one("meta[itemprop='startDate']")
    eventdate = ""
    eventtime = ""

    if meta and meta.get("content"):
        raw = meta["content"].strip()  # "2025-12-30 14:00"
        parts = raw.split()
        if len(parts) >= 1:
            eventdate = parts[0]
        if len(parts) >= 2:
            eventtime = parts[1]

    txt = _full_text(soup)
    if not eventdate:
        m = re.search(r"(\d{4}-\d{2}-\d{2})", txt)
        if m:
            eventdate = m.group(1)
    if not eventtime:
        m = re.search(r"(\d{1,2}[:â€¤]\d{2})", txt)
        if m:
            eventtime = m.group(1).replace("â€¤", ":")

    return eventdate[:32], eventtime[:32]


def _parse_event_venue(soup: BeautifulSoup) -> str:
    el = soup.select_one(".occurrence_venue span[itemprop='name']")
    if el:
        return _safe_text(el)[:100]

    txt = _full_text(soup)
    m = re.search(
        r"(Õ©Õ¡Õ¿Ö€Õ¸Õ¶|Õ¡Õ¯Õ¸Ö‚Õ´Õ¢|Õ»Õ¡Õ¦ Õ¡Õ¯Õ¸Ö‚Õ´Õ¢|Õ°Õ¡Õ´Õ¥Ö€Õ£Õ¡Õ½Ö€Õ¡Õ°|cinema|hall)[^\n]{0,80}",
        txt,
        re.IGNORECASE,
    )
    return m.group(0).strip()[:100] if m else ""


def _parse_event_price(soup: BeautifulSoup) -> str:
    meta_price = soup.select_one(
        "span[itemprop='offers'] meta[itemprop='price'], meta[itemprop='price']"
    )
    if meta_price and meta_price.get("content"):
        raw = meta_price["content"].strip()
        m = re.match(r"(\d+)", raw)
        if m:
            return m.group(1)

    txt = _full_text(soup)
    m = re.search(r"(\d{3,}(?:[-â€“]\d{3,})?)\s*(?:Õ¤Ö€\.?|Õ¤Ö€Õ¡Õ´|AMD)", txt)
    return m.group(1).replace("â€“", "-") if m else ""


def _parse_event_image(soup: BeautifulSoup) -> Optional[str]:
    og = soup.select_one("meta[property='og:image']")
    if og and og.get("content"):
        return og["content"].strip()

    img = soup.select_one(".event_photo img")
    if img and img.get("src"):
        src = img["src"].strip()
        return src if src.startswith("http") else BASE_TOMSARKGH_URL + src

    return None


def _parse_event_description(soup: BeautifulSoup) -> str:
    desc = soup.select_one(".description #eventDesc, .description span#eventDesc")
    if not desc:
        desc = soup.select_one(".description")
    text = desc.decode_contents() if desc else ""
    if text:
        text = BeautifulSoup(text, "html.parser").get_text("\n", strip=True)
    return text[:4000]


def scrape_tomsarkgh_event(
    url: str,
    base_category: str,
    event_type: Optional[int] = None,
) -> bool:
    """Scrape single event page (HY + optional EN) and save to DB."""
    try:
        logger.info(f"ğŸ« Scraping event: {url}")
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ---------- HY VERSION ----------
        title_el = soup.select_one("h1.event-name") or soup.select_one("h1")
        title_hy = _safe_text(title_el)[:200] or "Õ„Õ«Õ»Õ¸ÖÕ¡Õ¼Õ¸Ö‚Õ´"

        content_hy = _parse_event_description(soup)
        eventdate, eventtime = _parse_event_datetime(soup)
        venue_hy = _parse_event_venue(soup)
        price_hy = _parse_event_price(soup)
        image_url = _parse_event_image(soup)

        # ---------- EN VERSION (optional) ----------
        title_en = title_hy
        content_en = content_hy

        try:
            if "/hy/event" in url:
                url_en = url.replace("/hy/event", "/en/event")
            elif "/en/event" in url:
                url_en = url
            else:
                url_en = url.replace("/hy/", "/en/")

            resp_en = requests.get(url_en, headers=HEADERS, timeout=10)
            resp_en.raise_for_status()
            soup_en = BeautifulSoup(resp_en.text, "html.parser")

            title_en_el = soup_en.select_one("h1.event-name") or soup_en.select_one("h1")
            en_title = _safe_text(title_en_el)
            if en_title:
                title_en = en_title[:200]

            desc_en = soup_en.select_one(".description #eventDesc, .description, article, .content")
            if desc_en:
                text_en = BeautifulSoup(
                    desc_en.decode_contents(), "html.parser"
                ).get_text("\n", strip=True)
                if text_en:
                    content_en = text_en[:4000]
        except Exception:
            logger.debug(f"EN version unavailable for {url}")

        # ---------- CATEGORY FINAL ----------
        final_category = final_category_from_source(base_category, title_hy, content_hy)

        # ---------- SAVE ----------
        save_news(
            title_hy=title_hy,
            title_en=title_en,
            content_hy=content_hy,
            content_en=content_en,
            image_url=image_url,
            category=final_category,
            source_url=url,
            eventdate=eventdate,
            eventtime=eventtime,
            venue_hy=venue_hy,
            price_hy=price_hy,
        )

        logger.info(
            f"SAVED [{final_category}] {title_hy[:40]} | ğŸ“…{eventdate} â°{eventtime} "
            f"ğŸ“{venue_hy[:20]} ğŸ’°{price_hy}"
        )
        return True

    except Exception as e:
        logger.error(f"âŒ Event error: {url} â€” {e}")
        return False


# =============================================================================
# MAIN TOMSARKGH SCRAPER â€” FULL FLOW
# =============================================================================

def scrape_tomsarkgh_events() -> int:
    """Scrape all mapped Tomsarkgh categories."""
    logger.info("â–¶ï¸ Starting Tomsarkgh scraper (event pages)")
    total_saved = 0

    for event_type, base_category in TOMSARKGH_CATEGORIES.items():
        logger.info(f"ğŸ“‚ Category={base_category}, type={event_type}")
        links = fetch_tomsarkgh_events(event_type)

        if not links:
            logger.warning(f"âš ï¸ No events for type={event_type}")
            continue

        saved_for_type = 0
        for url in links:
            if scrape_tomsarkgh_event(url, base_category, event_type=event_type):
                saved_for_type += 1

        logger.info(
            f"âœ… {base_category} (type={event_type}): {saved_for_type}/{len(links)} saved"
        )
        total_saved += saved_for_type

    logger.info(f"âœ… === TOMSARKGH SCRAPER COMPLETE: {total_saved} items ===")
    return total_saved


# =============================================================================
# MAIN RUNNER
# =============================================================================

def run_all_scrapers() -> int:
    """Run complete news scraping cycle (Tomsarkgh + PanARMENIAN)."""
    logger.info("ğŸš€ === NEWS SCRAPER START ===")

    total = 0

    # 1) Tomsarkgh
    try:
        total_hy = scrape_tomsarkgh_events()
        total += total_hy
    except Exception as e:
        logger.error(f"Tomsarkgh scraper failed: {e}")

    logger.info(f"ğŸ === NEWS SCRAPER DONE: {total} items ===")
    return total


if __name__ == "__main__":
    run_all_scrapers()
